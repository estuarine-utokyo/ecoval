{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of point_layer template_title using point observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# bin_value using function from r4ecology's github\n",
    "import numpy as np\n",
    "def bin_value(x, bin_res):\n",
    "    return np.floor((x + bin_res / 2) / bin_res + 0.5) * bin_res - bin_res / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "chunk_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "variable = \"point_variable\".lower()\n",
    "vv_name = variable\n",
    "if vv_name in [\"benbio\", \"carbon\"]:\n",
    "    compact = True\n",
    "if vv_name.lower() == \"ph\":\n",
    "    vv_name = \"pH\"\n",
    "if vv_name in [\"doc\", \"poc\"]:\n",
    "    vv_name = vv_name.upper()\n",
    "if vv_name == \"benbio\":\n",
    "    vv_name = \"biomass of macrobenthos\"\n",
    "layer = \"point_layer\"\n",
    "# get the units. File inspection could be randomized in case people have put loose files in there...\n",
    "import glob\n",
    "df = pd.read_csv(\"../../matched/mapping.csv\")\n",
    "df = df.query(\"variable == @variable\")\n",
    "pattern = list(df.pattern)[0]\n",
    "paths = pd.read_csv(glob.glob(f\"../../matched/point/**/{layer}/{variable}/paths.csv\")[0]).path\n",
    "\n",
    "unit = None\n",
    "if unit is None:\n",
    "    try:\n",
    "        ff = glob.glob(f\"../../matched/point/**/{layer}/{variable}/*_{variable}_unit.csv\")[0]\n",
    "        df = pd.read_csv(ff)\n",
    "        unit = df.unit[0]\n",
    "    except:\n",
    "        pass\n",
    "if unit is None:\n",
    "    for ff in paths:\n",
    "        try:\n",
    "            ds = nc.open_data(paths[0])\n",
    "            model_variable = list(df.model_variable)[0].split(\"+\")[0]\n",
    "            unit = list(ds.contents.query(\"variable == @model_variable\").unit)[0]\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "if unit is None:\n",
    "    unit = \"unknown unit\"\n",
    "\n",
    "if variable == \"carbon\":\n",
    "    unit = \"kg m-3\"\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "ff = glob.glob(f\"../../matched/point/**/{layer}/{variable}/*_{variable}.csv\")[0]\n",
    "vv_source = os.path.basename(ff).split(\"_\")[0]\n",
    "vv_source = vv_source.upper()\n",
    "df = pd.read_csv(ff)\n",
    "if variable == \"ph\":\n",
    "    df = df.query(\"observation > 4\").reset_index(drop = True)\n",
    "# Danish part is always dubious\n",
    "df = df.query(\"lon < 9\")\n",
    "df_locs = df.loc[:,[\"lon\", \"lat\"]].drop_duplicates()\n",
    "# bin to 0.01 resolution\n",
    "df_raw = df\n",
    "df[\"lon\"] = df[\"lon\"].apply(lambda x: bin_value(x, 0.5))\n",
    "df[\"lat\"] = df[\"lat\"].apply(lambda x: bin_value(x, 0.5))\n",
    "if variable == \"benbio\":\n",
    "    df = df.assign(observation = lambda x: 1000 * 0.45 * x.observation) \n",
    "if variable not in  [\"carbon\", \"benbio\"]:\n",
    "    df = df.groupby([\"lon\", \"lat\", \"year\", \"month\"]).mean().reset_index()\n",
    "else:\n",
    "    df = df.groupby([\"lon\", \"lat\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"carbon\":\n",
    "    md(\"**Note**: This is in progress. Model and observation data are yet to be converted to comparable units!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "intro = []\n",
    "\n",
    "if vv_source == \"ICES\": \n",
    "\n",
    "    if layer == \"bottom\":\n",
    "        intro.append(f\"Near-bottom values of {vv_name} were extracted from ICES bottle and CTD data.\")\n",
    "    if layer == \"surface\":\n",
    "        intro.append(f\"Values from the **top 5 m** of the water column were extracted from ICES bottle and CTD data.\")\n",
    "    if layer == \"benthic\":\n",
    "        intro.append(\"Benthic values were extracted from existing datasets\")\n",
    "else:\n",
    "    if layer == \"bottom\":\n",
    "        intro.append(f\"This data was extracted from vertical profiles. The near-bottom value was defined as the value closest to the bottom, that was within 5 m of the bottom. Bathymetry was estimated using GEBCO Bathymetry data.\")\n",
    "    if layer == \"surface\":\n",
    "        intro.append(f\"This data was extracted from vertical profiles. Values from the **top 5 m** were extracted from the database. This was compared with the model values from the surface level.\")\n",
    "    if variable in [\"benbio\"]:\n",
    "        intro.append(\"Biomass data for macrobenthos was downloaded from the North Sea Benthos Survey 1986.\")\n",
    "\n",
    "if variable in [\"carbon\"]:\n",
    "    intro.append(\"Carbon data was compiled from multiple sources\")\n",
    "md(f\"In total there were {len(df_raw)} {layer} values extracted from the observational database.\")\n",
    "\n",
    "if layer == \"bottom\":\n",
    "    intro.append(\"**Note:** this analysis has been restricted to observations on the shelf region.\")\n",
    "\n",
    "\n",
    "if variable == \"poc\":\n",
    "    intro.append(\"Particulate organic carbon data was compiled from multiple sources\")\n",
    "\n",
    "if variable == \"pco2\":\n",
    "    intro.append(\"The variable pCO2water_SST_wet was extracted from the SOCAT 2023 database.\")\n",
    "    intro.append(\"Observational values were averaged for each day in the year.\")\n",
    "\n",
    "if variable == \"doc\":\n",
    "    intro.append(\"Dissolved organic carbon data was compiled from multiple sources\")\n",
    "\n",
    "df_mapping = pd.read_csv(\"../../matched/mapping.csv\")\n",
    "model_variable = list(df_mapping.query(\"variable == @variable\").model_variable)[0]\n",
    "\n",
    "if \"year\" in df_raw.columns:\n",
    "    min_year = df_raw.year.min()\n",
    "    max_year = df_raw.year.max()\n",
    "    if min_year == max_year:\n",
    "        intro.append(f\"The model output was matched up with the observational data for the year **{min_year}**.\")\n",
    "    else:\n",
    "        intro.append(f\"The model output was matched up with the observational data for the years **{min_year} to {max_year}**.\")\n",
    "\n",
    "intro.append(f\"The following model output was used to compare with observational **{vv_name}**: **{model_variable}**.\")\n",
    "\n",
    "md(\" \".join(intro).strip().replace(\"  \", \" \"))\n",
    "\n",
    "md(f\"The map below shows the locations of the matched up data for {vv_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# bottom 1% of observations\n",
    "bot_low = df.observation.quantile(0.001)\n",
    "df = df.query(f\"observation >= {bot_low}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i df_locs -i variable -i unit -w 500\n",
    "library(dplyr, warn.conflicts = FALSE)\n",
    "library(ggplot2, warn.conflicts = FALSE)\n",
    "library(stringr)\n",
    "world_map <- map_data(\"world\")\n",
    "# get lon, lat limits from profile_mld\n",
    "\n",
    "xlim = c(min(df_locs$lon), max(df_locs$lon))\n",
    "ylim = c(min(df_locs$lat), max(df_locs$lat))\n",
    "\n",
    "\n",
    "\n",
    "if(variable == \"temperature\"){\n",
    "    if(str_detect(unit, \"C\"))\n",
    "     unit = \"°C\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "gg <- df_locs %>%\n",
    "# final six months of the year\n",
    "    ggplot()+\n",
    "    geom_point(aes(lon, lat))+\n",
    "    theme_gray(base_size = 14)+\n",
    "    # add colour scale. Minimum zero, label 100, \">100\"\n",
    "    geom_polygon(data = world_map, aes(long, lat, group = group), fill = \"grey60\")+\n",
    "    coord_fixed(xlim = xlim, ylim = ylim, ratio = 1.5) \n",
    "\n",
    "# figure out if lon minimum is less than -10\n",
    "if( min(df_locs$lon) < -10 ){\n",
    "    # add sensible labels for longitude and latitude\n",
    "\n",
    "    gg <- gg +\n",
    "    scale_x_continuous(breaks = seq(-10, 5, 5), labels = c(\"10°W\", \"5°W\", \"0°\", \"5°E\"))+ \n",
    "    scale_y_continuous(breaks = seq(45, 60, 5), labels = c(\"45°N\", \"50°N\", \"55°N\", \"60°N\"))+\n",
    "    labs(x = \"\", y = \"\") \n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "    # move legen\n",
    "\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if layer == \"surface\":\n",
    "    md(f\"**Figure {chapter}{i_figure}:** Locations of matchups between simulated and observed {vv_name} in the top 5 m of the water column.\") \n",
    "if layer == \"bottom\":\n",
    "    md(f\"**Figure {chapter}{i_figure}:** Locations of matchups between simulated and observed {vv_name} near the bottom of the water column.\")\n",
    "if layer == \"all\":\n",
    "    md(f\"**Figure {chapter}{i_figure}:** Locations of matchups between simulated and observed {vv_name} throughout the water column.\")\n",
    "i_figure = i_figure + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i df -i variable -i unit -i layer -i vv_name -w 500 \n",
    "options(warn=-1)\n",
    "library(tidyverse)\n",
    "if ((\"month\" %in% colnames(df)) == FALSE){\n",
    "\n",
    "df_map <- df %>%\n",
    "    gather(variable, value, model:observation)\n",
    "    df_map\n",
    "# calculate the 98th percentil of the data\n",
    "p98 = quantile(df_map$value, 0.98)\n",
    "# cap the value at this\n",
    "df_map$value = pmin(df_map$value, p98)\n",
    "\n",
    "world_map <- map_data(\"world\")\n",
    "\n",
    "\n",
    "if(variable == \"temperature\"){\n",
    "    if(str_detect(unit, \"C\"))\n",
    "     unit = \"°C\"\n",
    "}\n",
    "\n",
    "\n",
    "Layer <- str_to_title(layer)\n",
    "name <- str_glue(\"{Layer} {vv_name} ({unit})\")\n",
    "\n",
    "\n",
    "gg <- df_map %>%\n",
    "    ggplot()+\n",
    "    geom_tile(aes(lon, lat, fill = value))+\n",
    "    theme_gray(base_size = 14)+\n",
    "    coord_fixed(ratio = 1.5, xlim = c(min(df$lon), max(df$lon)), ylim = c(min(df$lat), max(df$lat)))+\n",
    "    labs(color = variable)+\n",
    "    # log10\n",
    "    scale_color_viridis_c()+\n",
    "    theme(legend.position = \"bottom\")+\n",
    "    facet_wrap(~variable)+\n",
    "      scale_fill_viridis_c(\n",
    "        # use unit for the label\n",
    "        name = name,\n",
    "                       guide = guide_colorbar(title.position = \"bottom\", title.hjust = 0.5, title.theme = element_text(angle = 0, size = 20, family = \"Helvetica\"))\n",
    "  )+\n",
    "    theme(\n",
    "    legend.position = \"bottom\", legend.direction = \"horizontal\", legend.box = \"horizontal\", legend.key.width = unit(6.0, \"cm\"),\n",
    "    legend.key.height = unit(1.0, \"cm\"))\n",
    "\n",
    "\n",
    "y_labels <-  as.numeric(na.omit(layer_scales(gg)$y$break_positions()))\n",
    "x_labels <- as.numeric(na.omit(layer_scales(gg)$x$break_positions()))\n",
    "x_breaks <- x_labels\n",
    "y_breaks <- y_labels\n",
    "\n",
    "# y labels are north-south coordinates. Make them more appropriate\n",
    "# i.e. 10 should be 10 °N, -10 should be 10 °S\n",
    "\n",
    "y_labels <- ifelse(y_labels >= 0, paste0(y_labels, \"°N\"), paste0(abs(y_labels), \"°S\"))\n",
    "x_labels <- ifelse(x_labels >= 0, paste0(x_labels, \"°E\"), paste0(abs(x_labels), \"°W\"))\n",
    "\n",
    "gg <- gg + scale_x_continuous(breaks = x_breaks, labels = x_labels) + scale_y_continuous(breaks = y_breaks, labels = y_labels)+\n",
    "    geom_polygon(data = world_map, aes(long, lat, group = group), fill = \"grey60\")\n",
    "\n",
    "# remove x and y axis names\n",
    "gg <- gg +\n",
    "    labs(x = \"\", y = \"\") \n",
    "\n",
    "# ditch the whitespace around the plot\n",
    "gg <- gg + theme(plot.margin=unit(c(0,0,0,0),\"cm\"))\n",
    "gg\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if \"month\" not in df.columns:\n",
    "    md(f\"**Figure {chapter}{i_figure}:** Map of average {layer} {vv_name} in the model and observational datasets.\")\n",
    "    i_figure += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if \"month\" in df_raw.columns:\n",
    "    # summarize using md the number of observations in each month\n",
    "    # get the minimum and maximum number in each month and report the month\n",
    "    df_size = df_raw.groupby(\"month\").size().reset_index()\n",
    "    df_size.columns = [\"month\", \"n\"]\n",
    "    n_min = df_size.n.min()\n",
    "    n_max = df_size.n.max()\n",
    "    month_min = list(df_size.query(\"n == @n_min\").month.values)[0]\n",
    "    months_max = list(df_size.query(\"n == @n_max\").month.values)[0] \n",
    "    # convert to month names\n",
    "    import calendar\n",
    "    month_min = calendar.month_name[int(month_min)]\n",
    "    months_max = calendar.month_name[int(months_max)] \n",
    "    \n",
    "    # summarize using md\n",
    "    \n",
    "    fig_summary = [f\"The number of observations in each month ranged from {n_min} in {month_min} to {n_max} in {months_max}.\"]\n",
    "    \n",
    "    fig_summary.append(f\"Figure {chapter}{i_figure} below shows the distribution of observations in each month.\")\n",
    "    \n",
    "    md(\" \".join(fig_summary).strip().replace(\"  \", \" \"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i df_raw -i variable -i unit -w 500 \n",
    "# calculate number of observations per month\n",
    "# figure out if \"month\" in df\n",
    "if(\"month\" %in% colnames(df_raw)){\n",
    "\n",
    "df1 <- df_raw %>%\n",
    "    group_by(lon, lat, month) %>%\n",
    "    summarise(observation = n()) %>%\n",
    "    ungroup()   \n",
    "\n",
    "# plot number of observations per month using plotnine and geom_bar\n",
    "\n",
    "# change month to month name\n",
    "df1$month <- factor(df1$month, levels = 1:12, labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))\n",
    "\n",
    "gg <- df1 %>%\n",
    "    ggplot(aes(x = month, y = observation))+\n",
    "    theme_gray(base_size = 14)+\n",
    "    geom_bar(stat = \"identity\")+\n",
    "    labs(y = \"Number of observations\", x= \"\")\n",
    "\n",
    "gg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if \"month\" in df.columns:\n",
    "    md(f\"**Figure {chapter}{i_figure}:** Number of {layer} observations per month for {vv_name}.\")\n",
    "    i_figure += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "bias_text = []\n",
    "\n",
    "bias_text.append(f\"Figure {chapter}{i_figure} below shows the bias between the model and observational data for {vv_name}.\")\n",
    "bias_text.append(f\"The bias is calculated as the model value minus the observational value, and it is shown for each month of the year.\")\n",
    "\n",
    "md(\" \".join(bias_text).strip().replace(\"  \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i df -i variable -i unit -i layer -w 1000 -h 1200 -i vv_name\n",
    "options(warn=-1)\n",
    "#%%R -i df -i variable -i unit -w 1600 -h 1000\n",
    "options(warn=-1)\n",
    "\n",
    "library(dplyr, warn.conflicts = FALSE)\n",
    "library(ggplot2, warn.conflicts = FALSE)\n",
    "library(stringr)\n",
    "library(tidyverse)\n",
    "world_map <- map_data(\"world\")\n",
    "# get lon, lat limits from profile_mld\n",
    "\n",
    "xlim = c(min(df$lon), max(df$lon))\n",
    "ylim = c(min(df$lat), max(df$lat))\n",
    "\n",
    "\n",
    "if(vv_name == \"temperature\"){\n",
    "    if(str_detect(unit, \"C\"))\n",
    "     unit = \"°C\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df <- df %>%\n",
    "    mutate(bias = model - observation) \n",
    "\n",
    "# calculate the absolate bias\n",
    "\n",
    "df1 <- df %>%\n",
    "    mutate(bias = abs(bias))\n",
    "# calculate the 98th percentile of the absolute bias\n",
    "bias_high <- df1$bias %>% quantile(0.98)\n",
    "# cap the bias to +/1 98th percentile\n",
    "df$bias[df$bias > bias_high] <- bias_high\n",
    "df$bias[df$bias < -bias_high] <- -bias_high\n",
    "\n",
    "\n",
    "\n",
    "plot_month <- FALSE\n",
    "if(\"month\" %in% colnames(df))\n",
    "    plot_month <- TRUE\n",
    "\n",
    "# # convert month number to month in profile_mld\n",
    "if(plot_month){\n",
    "    df <- df %>%\n",
    "        arrange(month)\n",
    "df$month <- factor(df$month, levels = df$month, labels = month.abb[df$month])\n",
    "}\n",
    "# df$month <- factor(df$month, labels = month.abb)\n",
    "\n",
    "title <- str_glue(\"Bias in {layer} {vv_name} ({unit})\")\n",
    "\n",
    "out = str_glue(\"../../results/{layer}/{variable}/{layer}_{variable}_bias.csv\")\n",
    "\n",
    "# # check directory exists for out\n",
    "if (!dir.exists(dirname(out))){\n",
    "    dir.create(dirname(out), recursive = TRUE)\n",
    "}\n",
    "df %>% write_csv(out)\n",
    "\n",
    "# df.to_csv(out, index = False)\n",
    "\n",
    "# export to csv\n",
    "\n",
    "title = str_replace(title, \"/m\\\\^3\", \"m<sup>-3\")\n",
    "\n",
    "\n",
    "\n",
    "gg <- df %>%\n",
    "# final six months of the year\n",
    "    ggplot()+\n",
    "    geom_raster(aes(lon, lat, fill = bias))+\n",
    "    theme_gray(base_size = 24)+\n",
    "    # add colour scale. Minimum zero, label 100, \">100\"\n",
    "    coord_fixed(xlim = xlim, ylim = ylim, ratio = 1.5) +\n",
    "    # move legend to the top. Make it 3 cm wide\n",
    "    # move legend title to the bottom and centre it\n",
    "    scale_fill_gradient2(low = \"blue\", high = \"red\",\n",
    "                       guide = guide_colorbar(title.position = \"bottom\", title.hjust = 0.5, title.theme = ggtext::element_markdown(angle = 0, size = 20, family = \"Helvetica\"))\n",
    "                    #    guide = guide_colorbar(title.position = \"bottom\", title.hjust = 0.5, title.theme = element_text(angle = 0, size = 20, family = \"Helvetica\"))\n",
    "  )+\n",
    "    theme(\n",
    "    legend.position = \"bottom\", legend.direction = \"horizontal\", legend.box = \"horizontal\", legend.key.width = unit(6.0, \"cm\"),\n",
    "    # legend.title = ggtext::element_markdown(),\n",
    "    legend.key.height = unit(1.0, \"cm\"))+\n",
    "    # set the legend title to bias\n",
    "    labs(fill = title)\n",
    "  #  .title.x = ggtext::element_markdown())\n",
    "\n",
    "if (plot_month){\n",
    "    gg <- gg + facet_wrap(~month)\n",
    "}\n",
    "\n",
    "\n",
    "y_labels <-  as.numeric(na.omit(layer_scales(gg)$y$break_positions()))\n",
    "x_labels <- as.numeric(na.omit(layer_scales(gg)$x$break_positions()))\n",
    "x_breaks <- x_labels\n",
    "y_breaks <- y_labels\n",
    "\n",
    "# y labels are north-south coordinates. Make them more appropriate\n",
    "# i.e. 10 should be 10 °N, -10 should be 10 °S\n",
    "\n",
    "y_labels <- ifelse(y_labels >= 0, paste0(y_labels, \"°N\"), paste0(abs(y_labels), \"°S\"))\n",
    "x_labels <- ifelse(x_labels >= 0, paste0(x_labels, \"°E\"), paste0(abs(x_labels), \"°W\"))\n",
    "\n",
    "gg <- gg + scale_x_continuous(breaks = x_breaks, labels = x_labels) + scale_y_continuous(breaks = y_breaks, labels = y_labels)+\n",
    "    geom_polygon(data = world_map, aes(long, lat, group = group), fill = \"grey60\")\n",
    "\n",
    "gg <- gg +\n",
    "    labs(x = \"\", y = \"\")\n",
    "\n",
    "    # move legen\n",
    "\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"**Figure {chapter}{i_figure}**: Bias in {layer} {vv_name}. The bias is calculated as model - observation. The colour scale is from blue (negative bias) to red (positive bias). The colour scale is capped at the 98th percentile of the absolute bias. This is to avoid a few extreme outliers from dominating the colour scale. **Note:** values have been binned and averaged to 0.5 degree resolution.\") \n",
    "i_figure += 1\n",
    "\n",
    "#\"adhoc/tmp/df_raw.feather\"\n",
    "# create directory if non-existent, recursive\n",
    "if os.path.isdir(\"adhoc/tmp\") == False:\n",
    "    os.makedirs(\"adhoc/tmp\")\n",
    "df_raw.to_feather(\"adhoc/tmp/df_raw.feather\")\n",
    "df.to_feather(\"adhoc/tmp/df.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "scatter_text = []\n",
    "scatter_text.append(f\"Figures {chapter}{i_figure} and {chapter}{i_figure + 1} show the distribution of {layer} {vv_name} observations in the model and observational datasets.\") \n",
    "scatter_text.append(f\"This is shown for each month of the year (Figure {chapter}{i_figure}) and for the entire year (Figure {chapter}{i_figure + 1}).\")\n",
    "\n",
    "md(\" \".join(scatter_text).strip().replace(\"  \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i df -i compact -i vv_name -i unit -w 1000 -h 1200\n",
    "#%%R -i df -i variable -i unit -w 1600 -h 1000\n",
    "#df <- arrow::read_feather(\"adhoc/tmp/df_raw.feather\")\n",
    "if(\"month\" %in% colnames(df) & compact == FALSE){\n",
    "\n",
    "\n",
    "library(tidyverse, warn.conflicts = FALSE)\n",
    "\n",
    "if(vv_name == \"temperature\"){\n",
    "    if(str_detect(unit, \"C\"))\n",
    "     unit = \"°C\"\n",
    "}\n",
    "\n",
    "x_lab <- str_glue(\"Model {vv_name} ({unit})\")\n",
    "y_lab <- str_glue(\"Observed {vv_name} ({unit})\")\n",
    "\n",
    "\n",
    "x_lab <- str_replace(x_lab, \"/m\\\\^3\", \"m<sup>-3</sup>\")\n",
    "y_lab <- str_replace(y_lab, \"/m\\\\^3\", \"m<sup>-3</sup>\")\n",
    "\n",
    "df <- df %>%\n",
    "# convert month number to name, e.g. 1=Jan\n",
    "# do not use a factor\n",
    "    mutate(month = month.abb[month]) %>%\n",
    "    ungroup()\n",
    "\n",
    "df <- df %>%\n",
    "    mutate(month = \"All months\") %>%\n",
    "    ungroup() %>%\n",
    "    bind_rows(df)\n",
    "\n",
    "# convert month to factor\n",
    "df$month <- factor(df$month, levels = c(\"All months\", month.abb))\n",
    "\n",
    "\n",
    "\n",
    "gg <- df %>%\n",
    "# final six months of the year\n",
    "    ggplot()+\n",
    "    geom_point(aes(model, observation))+\n",
    "    facet_wrap(~month)+\n",
    "    theme_gray(base_size = 24)+\n",
    "    labs(fill = title)+\n",
    "    geom_abline()+\n",
    "    geom_smooth(aes(model, observation), method = \"gam\")+\n",
    "    labs(x = x_lab, y = y_lab)+\n",
    "    theme(axis.title.x = ggtext::element_markdown())+\n",
    "    theme(axis.title.y = ggtext::element_markdown())\n",
    "\n",
    "    # move legen\n",
    "\n",
    "gg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    if compact is False:\n",
    "        if layer == \"surface\":\n",
    "            md(f\"**Figure {chapter}{i_figure}**: Simulated versus observed {vv_name} in the top 5 m of the water column. The blue curve is a generalized additive model fit to the data, and the black line represents 1-1 relationship between the simulation and observations. The data has been binned to 0.5 degree resolution.\") \n",
    "        if layer == \"bottom\":\n",
    "            md(f\"**Figure {chapter}{i_figure}**: Simulated versus observed {vv_name} near the bottom of the water column. The blue curve is a generalized additive model fit to the data, and the black line represents 1-1 relationship between the simulation and observations. The data has been binned to 0.5 degree resolution.\")\n",
    "        if layer == \"all\":\n",
    "            md(f\"**Figure {chapter}{i_figure}**: Simulated versus observed {vv_name} throughout the water column. The blue curve is a generalized additive model fit to the data, and the black line represents 1-1 relationship between the simulation and observations. The data has been binned to 0.5 degree resolution.\")\n",
    "        i_figure = i_figure + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i vv_name -i unit -i compact -w 500 \n",
    "\n",
    "if(compact){\n",
    "library(dplyr, warn.conflicts = FALSE)\n",
    "library(ggplot2, warn.conflicts = FALSE)\n",
    "library(stringr)\n",
    "\n",
    "\n",
    "df <- arrow::read_feather(\"adhoc/tmp/df.feather\")\n",
    "\n",
    "\n",
    "\n",
    "x_lab <- str_glue(\"Model {vv_name} ({unit})\")\n",
    "y_lab <- str_glue(\"Observed {vv_name} ({unit})\")\n",
    "x_lab <- str_replace(x_lab, \"/m\\\\^3\", \"m<sup>-3</sup>\")\n",
    "y_lab <- str_replace(y_lab, \"/m\\\\^3\", \"m<sup>-3</sup>\")\n",
    "\n",
    "\n",
    "gg <- df %>%\n",
    "# final six months of the year\n",
    "    ggplot()+\n",
    "    geom_point(aes(model, observation))+\n",
    "    theme_gray(base_size = 14)+\n",
    "    labs(fill = title)+\n",
    "    geom_abline()+\n",
    "    geom_smooth(aes(model, observation), method = \"gam\")+\n",
    "    labs(x = x_lab, y = y_lab)+\n",
    "    theme(axis.title.x = ggtext::element_markdown())+\n",
    "    theme(axis.title.y = ggtext::element_markdown())\n",
    "    # move legen\n",
    "\n",
    "gg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if compact:\n",
    "    md(f\"**Figure {chapter}{i_figure}**: Model vs observed {vv_name} for {layer} values. The observations are from {vv_source}. The line is a GAM fit to the data. The shaded area is the 95% confidence interval of the GAM fit.\")\n",
    "    i_figure += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"The overall ability of the model to predict the observed {vv_name} was assessed by calculating the average bias, the root mean square error (RMSE) and the correlation coefficient (R). The bias was calculated as the model value minus the observed value. The RMSE was calculated as the square root of the mean squared error. The correlation coefficient was calculated as the Pearson correlation coefficient between the model and observed values.\") \n",
    "md(f\"This was calculated for each month and for the entire dataset. The results are shown in the tables below.\")\n",
    "md(f\"This is calculated in two separate ways. First, we use the raw model and observed values. Second, we use data that was averaged to 0.5 to 0.5 bins to account for spatial bias.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_bias = (\n",
    "        df_raw\n",
    "        .assign(bias = lambda x: x.model - x.observation)\n",
    "        .groupby(\"month\")\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .loc[:,[\"month\", \"bias\"]]\n",
    "        # convert month number to name\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "    )\n",
    "    # add average bias to df_bias as a separate row\n",
    "    annual_bias = df_raw.model.mean() - df_raw.observation.mean() \n",
    "    df_bias = pd.concat([df_bias, pd.DataFrame({\"month\": [\"All\"], \"bias\": [annual_bias]})])\n",
    "\n",
    "    # move the final row to the top\n",
    "    df_bias = pd.concat([df_bias.iloc[[-1]], df_bias.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_bias = pd.DataFrame({\"month\": [\"All\"], \"bias\": [df_raw.model.mean() - df_raw.observation.mean()]})\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    # now create an rmse dataframe\n",
    "    df_rmse = (\n",
    "        df_raw\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "        .groupby(\"month\")\n",
    "        .apply(lambda x: np.sqrt((x.model - x.observation).pow(2).mean()))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"rmse\"})\n",
    "    )\n",
    "    # add average rmse to df_rmse as a separate row\n",
    "    annual_rmse = np.sqrt(((df_raw.model - df_raw.observation).pow(2)).mean())\n",
    "    df_rmse = pd.concat([df_rmse, pd.DataFrame({\"month\": [\"All\"], \"rmse\": [annual_rmse]})])\n",
    "    # move the final row to the top\n",
    "    df_rmse = pd.concat([df_rmse.iloc[[-1]], df_rmse.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_rmse = pd.DataFrame({\"month\": [\"All\"], \"rmse\": [np.sqrt(((df_raw.model - df_raw.observation).pow(2)).mean())]})\n",
    "# rename the month column to Month\n",
    "# merge the two dataframes\n",
    "df_table = copy.deepcopy(df_bias).merge(df_rmse)\n",
    "df_table = df_table.round(2)\n",
    "# create df_corr\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_corr = (\n",
    "        df_raw\n",
    "        .groupby(\"month\")\n",
    "        .apply(lambda x: x.model.corr(x.observation))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"correlation\"})\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "    )\n",
    "    # add average correlation to df_corr as a separate row\n",
    "    # calculate annual correlation using all data\n",
    "    annual_corr = df_raw.model.corr(df_raw.observation)\n",
    "    df_corr = pd.concat([df_corr, pd.DataFrame({\"month\": [\"All\"], \"correlation\": [annual_corr]})])\n",
    "    # df_corr = df_corr.append({\"month\": \"All\", \"correlation\": annual_corr}, ignore_index=True)\n",
    "\n",
    "    # move the final row to the top\n",
    "    df_corr = pd.concat([df_corr.iloc[[-1]], df_corr.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_corr = pd.DataFrame({\"month\": [\"All\"], \"correlation\": [df_raw.model.corr(df_raw.observation)]})\n",
    "df_table = df_table.merge(df_corr)\n",
    "df_table = df_table.round(2)\n",
    "df_table = df_table.rename(columns={\"month\": \"Month\", \"bias\": \"Bias\", \"rmse\": \"RMSE\", \"correlation\": \"Correlation\"})\n",
    "df_table = df_table[[\"Month\", \"Bias\", \"RMSE\", \"Correlation\"]]\n",
    "# change Month to Period\n",
    "df_table = df_table.rename(columns={\"Month\": \"Time period\"})\n",
    "\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    # add commas to bias and rmse\n",
    "    df_number = df_raw.groupby(\"month\").count().reset_index().loc[:,[\"month\", \"observation\"]]\n",
    "# convert month number to name\n",
    "    df_number[\"month\"] = df_number[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "    df_number = df_number.rename(columns={\"month\": \"Time period\", \"observation\": \"Number of observations\"})\n",
    "else:\n",
    "    df_number = pd.DataFrame({\"Time period\": [\"All\"], \"Number of observations\": [len(df_raw)]})\n",
    "\n",
    "# add total number of observations\n",
    "annual_number = len(df_raw)\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_number = pd.concat([df_number, pd.DataFrame({\"Time period\": [\"All\"], \"Number of observations\": [annual_number]})])\n",
    "# df_number = df_number.append({\"Time period\": \"All\", \"Number of observations\": annual_number}, ignore_index=True)\n",
    "df_table = df_table.merge(df_number)\n",
    "\n",
    "# include commas in the number of observations\n",
    "df_table[\"Number of observations\"] = df_table[\"Number of observations\"].apply(lambda x: \"{:,}\".format(x))\n",
    "# convert nan to \"N/A\"\n",
    "df_table = df_table.fillna(\"N/A\")\n",
    "\n",
    "df_table.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"**Table {chapter}{i_table}:** Average bias and root-mean square error in {layer} {vv_name} for each month using the raw {vv_source} data. The bias is calculated as model - observation. The average bias is calculated as the mean of the monthly biases.\")\n",
    "i_table += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_bias = (\n",
    "        df\n",
    "        .assign(bias = lambda x: x.model - x.observation)\n",
    "        .groupby(\"month\")\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .loc[:,[\"month\", \"bias\"]]\n",
    "        # convert month number to name\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "    )\n",
    "    # add average bias to df_bias as a separate row\n",
    "    annual_bias = df.model.mean() - df.observation.mean() \n",
    "    df_bias = pd.concat([df_bias, pd.DataFrame({\"month\": [\"All\"], \"bias\": [annual_bias]})])\n",
    "\n",
    "    # move the final row to the top\n",
    "    df_bias = pd.concat([df_bias.iloc[[-1]], df_bias.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_bias = pd.DataFrame({\"month\": [\"All\"], \"bias\": [df.model.mean() - df.observation.mean()]})\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    # now create an rmse dataframe\n",
    "    df_rmse = (\n",
    "        df\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "        .groupby(\"month\")\n",
    "        .apply(lambda x: np.sqrt((x.model - x.observation).pow(2).mean()))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"rmse\"})\n",
    "    )\n",
    "    # add average rmse to df_rmse as a separate row\n",
    "    annual_rmse = np.sqrt(((df.model - df.observation).pow(2)).mean())\n",
    "    df_rmse = pd.concat([df_rmse, pd.DataFrame({\"month\": [\"All\"], \"rmse\": [annual_rmse]})])\n",
    "    # move the final row to the top\n",
    "    df_rmse = pd.concat([df_rmse.iloc[[-1]], df_rmse.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_rmse = pd.DataFrame({\"month\": [\"All\"], \"rmse\": [np.sqrt(((df.model - df.observation).pow(2)).mean())]})\n",
    "# rename the month column to Month\n",
    "# merge the two dataframes\n",
    "df_table = copy.deepcopy(df_bias).merge(df_rmse)\n",
    "df_table = df_table.round(2)\n",
    "# create df_corr\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_corr = (\n",
    "        df\n",
    "        .groupby(\"month\")\n",
    "        .apply(lambda x: x.model.corr(x.observation))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"correlation\"})\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "    )\n",
    "    # add average correlation to df_corr as a separate row\n",
    "    # calculate annual correlation using all data\n",
    "    annual_corr = df.model.corr(df.observation)\n",
    "    df_corr = pd.concat([df_corr, pd.DataFrame({\"month\": [\"All\"], \"correlation\": [annual_corr]})])\n",
    "    # df_corr = df_corr.append({\"month\": \"All\", \"correlation\": annual_corr}, ignore_index=True)\n",
    "\n",
    "    # move the final row to the top\n",
    "    df_corr = pd.concat([df_corr.iloc[[-1]], df_corr.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_corr = pd.DataFrame({\"month\": [\"All\"], \"correlation\": [df.model.corr(df.observation)]})\n",
    "df_table = df_table.merge(df_corr)\n",
    "df_table = df_table.round(2)\n",
    "df_table = df_table.rename(columns={\"month\": \"Month\", \"bias\": \"Bias\", \"rmse\": \"RMSE\", \"correlation\": \"Correlation\"})\n",
    "df_table = df_table[[\"Month\", \"Bias\", \"RMSE\", \"Correlation\"]]\n",
    "# change Month to Period\n",
    "df_table = df_table.rename(columns={\"Month\": \"Time period\"})\n",
    "\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    # add commas to bias and rmse\n",
    "    df_number = df.groupby(\"month\").count().reset_index().loc[:,[\"month\", \"observation\"]]\n",
    "# convert month number to name\n",
    "    df_number[\"month\"] = df_number[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "    df_number = df_number.rename(columns={\"month\": \"Time period\", \"observation\": \"Number of observations\"})\n",
    "else:\n",
    "    df_number = pd.DataFrame({\"Time period\": [\"All\"], \"Number of observations\": [len(df)]})\n",
    "\n",
    "# add total number of observations\n",
    "annual_number = len(df)\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_number = pd.concat([df_number, pd.DataFrame({\"Time period\": [\"All\"], \"Number of observations\": [annual_number]})])\n",
    "# df_number = df_number.append({\"Time period\": \"All\", \"Number of observations\": annual_number}, ignore_index=True)\n",
    "df_table = df_table.merge(df_number)\n",
    "\n",
    "# include commas in the number of observations\n",
    "df_table[\"Number of observations\"] = df_table[\"Number of observations\"].apply(lambda x: \"{:,}\".format(x))\n",
    "df_table = df_table.fillna(\"N/A\")\n",
    "\n",
    "df_table.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"**Table {chapter}{i_table}:** Average bias and root-mean square error of simulated {layer} {vv_name} for each month using geographically binned {vv_source} data. Data was averaged in each 0.5 by 0.5 degree cell in each year and month. The bias is calculated as model - observation. The average bias is calculated as the mean of the monthly biases.\")\n",
    "i_table += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regresion analysis of model vs observed point_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"A linear regression analysis of modelled and observed {vv_name} was performed. The modelled {vv_name} was used as the independent variable and the observed {vv_name} was used as the dependent variable. The results are shown in the table below.\")\n",
    "\n",
    "md(\"The regression was carried out using the Python package statsmodels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# do a linear regression of model vs observed in df\n",
    "X = df.model.values\n",
    "Y = df.observation.values\n",
    "# linear regression using statsmodels\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X)\n",
    "# make X and Y random numbers between 0 and 1\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X).fit()\n",
    "# get the slope and intercept\n",
    "intercept, slope = model.params\n",
    "# calculate the r squared\n",
    "r2 = model.rsquared\n",
    "# calculate the p value of the slope\n",
    "p = model.f_pvalue\n",
    "\n",
    "p = model.f_pvalue\n",
    "# put that in a dataframe\n",
    "df_stats = pd.DataFrame({\"Slope\": slope, \"Intercept\": intercept, \"R2\": r2, \"P\": p}, index = [\"All\"]).assign(Period = \"All\")\n",
    "# do this month by month append to df_stats\n",
    "\n",
    "for month in range(1, 13):\n",
    "    try:\n",
    "        X = df.query(\"month == @month\").model.values\n",
    "        Y = df.query(\"month == @month\").observation.values\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        intercept, slope = model.params\n",
    "        r2 = model.rsquared\n",
    "        p = model.f_pvalue\n",
    "        df_stats = pd.concat([df_stats, pd.DataFrame({\"Slope\": slope, \"Intercept\": intercept, \"R2\": r2, \"P\": p}, index = [month]).assign(Period = month)])\n",
    "        df_stats.loc[df_stats.index[-1], \"Period\"] = calendar.month_abbr[month]\n",
    "    except:\n",
    "        pass\n",
    "# sort period appropriately, so All is first then ordered by month\n",
    "df_stats[\"Period\"] = pd.Categorical(df_stats[\"Period\"], [calendar.month_abbr[x] for x in range(1, 13)] + [\"All\"])\n",
    "# round p-value to 3 dp\n",
    "df_stats[\"P\"] = df_stats[\"P\"].round(5)\n",
    "# change P to p-value\n",
    "df_stats = df_stats.rename(columns={\"P\": \"p-value\"})\n",
    "# put Period first\n",
    "df_stats = df_stats[[\"Period\", \"Slope\", \"Intercept\", \"R2\", \"p-value\"]]\n",
    "# \n",
    "\n",
    "df_stats.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"**Table {chapter}{i_table}:** Linear regression analysis of modelled and observed {vv_name}. The modelled {vv_name} was used as the independent variable and the observed {vv_name} was used as the dependent variable. The slope and intercept of the regression line are shown, along with the R<sup>2</sup> value and the p-value of the slope. The p-value is a measure of the significance of the slope. A p-value less than 0.05 is considered statistically significant. Note: only months with sufficient values for a regression are shown.\")\n",
    "i_table += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "chunk_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"poc\":\n",
    "    md(\"Boss, Emmanuel; Picheral, Marc; Searson, Sarah; Le Goff, Hervé; Reverdin, Gilles; Leeuw, Thomas; Chase, Alison P; Bricaud, Annick; Kolber, Zbigniew S; Taillandier, V; Pesant, Stephane; Tara Oceans Consortium, Coordinators; Tara Oceans Expedition, Participants (2017): Underway surface water data during the Tara Oceans expedition in 2009-2012 [dataset]. PANGAEA, https://doi.org/10.1594/PANGAEA.873566, In: Boss, Emmanuel; Picheral, Marc; Searson, Sarah; Marec, Claudie; Le Goff, Hervé; Reverdin, Gilles; Leeuw, Thomas; Chase, Alison P; Anderson, Leif G; Gattuso, Jean-Pierre; Pino, Diana Ruiz; Padín, Xose Antonio; Grondin, Pierre-Luc; Matuoka, Atsushi; Babin, Marcel; Bricaud, Annick; Kolber, Zbigniew S; Taillandier, V; Hafez, Mark; Chekalyuk, Alexander; Pesant, Stephane; Météo France; Tara Oceans Consortium, Coordinators (2017): Harmonised data from underway navigation, meteorology and surface water measurements during the Tara Oceans expedition in 2009-2013 [dataset publication series]. PANGAEA, https://doi.org/10.1594/PANGAEA.873592\")\n",
    "    md(\"Röttgers, Rüdiger; Bi, Shun; Burmester, Henning; Heymann, Kerstin; Hieronymi, Martin; Krasemann, Hajo; Schönfeld, Wolfgang (2023): Water inherent optical properties and concentrations of water constituents from the German Bight and adjacent regions: concentrations and auxiliary data [dataset]. PANGAEA, https://doi.org/10.1594/PANGAEA.950767, In: Röttgers, Rüdiger; Bi, Shun; Burmester, Henning; Heymann, Kerstin; Hieronymi, Martin; Krasemann, Hajo; Schönfeld, Wolfgang (2023): Water inherent optical properties and concentrations of water constituents from the German Bight and adjacent regions [dataset bundled publication]. PANGAEA, https://doi.org/10.1594/PANGAEA.950774)\")\n",
    "    md(\"Loisel, Hubert; Duforêt-Gaurier, Lucile; Tran, Trung Kien; Jorge, Daniel S F; Steinmetz, Francois; Mangin, Antoine; Bretagnon, Marine; d'Andon, Odile (2023): Database (DSM) of in situPOC, SPM and Rrs collected between 1997 and 2018 [dataset]. PANGAEA, https://doi.org/10.1594/PANGAEA.960962\")\n",
    "    md(\"Lønborg, Christian; Carreira, Cátia; Abril, Gwenael; Agustí, Susana; Amaral, Valentina; Andersson, Agneta; Arístegui, Javier; Bhadury, Punyasloke; Bif, Mariana B; Borges, Alberto Vieira; Bouillon, Steven; Calleja, Maria Ll; Cotovicz, Luiz C Jr; Cozzi, Stefano; Doval, Maryló; Duarte, Carlos Manuel; Eyre, Bradley D; Fichot, Cedric; García-Martín, Elena; Garzon-Garcia, Alexandra; Giani, Michele; Gonçalves-Araujo, Rafael; Gruber, Renee K; Hansell, Dennis A; Hashihama, Fuminori; He, Ding; Holding, Johnna M; Hunter, William Ross; Ibánhez, J Severino; Ibello, Valeria; Jiang, Shan; Kim, Guebuem; Klun, Katja; Kowalczuk, Piotr; Kubo, Atsushi; Lee, Choon Weng; Lopes, Claudia B; Maggioni, Federica; Magni, Paolo; Marrasé, Celia; Martin, Patrick; McCallister, S Leigh; McCallum, Rosh; M Medeiros, Patricia; G Morán, Xosé Anxelu; Muller-Karger, Frank; Myers-Pigg, Allison; Norli, Marit; Oakes, Joanne M; Osterholz, Helena; Park, Hyekyung; Lund Paulsen, Maria; Rosentreter, Judith A; Ross, Jeff; Rueda-Roa, Digna; Santinelli, Chiara; Shen, Yuan; Teira, Eva; Tinta, Tinkara; Uher, Guenther; Wakita, Masahide; Ward, Nicholas D; Watanabe, Kenta; Xin, Yu; Yamashita, Youhei; Yang, Liyang; Yeo, Jacob; Yuan, Huamao; Zheng, Qiang; Álvarez‐Salgado, Xosé Antón (2023): A global database of dissolved organic matter (DOM) concentration measurements in coastal waters (CoastDOM v.1) [dataset]. PANGAEA, https://doi.org/10.1594/PANGAEA.964012\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if vv_source.lower() == \"ices\":\n",
    "    md(\"ICES Data Portal, Dataset on Ocean HydroChemistry, Extracted March 3, 2023. ICES, Copenhagen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"carbon\":\n",
    "    md('Diesing, Markus, Terje Thorsnes, and Lilja Rún Bjarnadóttir. \"Organic carbon densities and accumulation rates in surface sediments of the North Sea and Skagerrak.\" Biogeosciences 18.6 (2021): 2139-2160.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if vv_source.lower() == \"socat23\":\n",
    "    md(\"Bakker, Dorothee C. E.; Alin, Simone R.; Bates, Nicholas; Becker, Meike; Feely, Richard A.; Gkritzalis, Thanos; Jones, Steve D.; Kozyr, Alex; Lauvset, Siv K.; Metzl, Nicolas; Munro, David R.; Nakaoka, Shin-ichiro; Nojiri, Yukihiro; O'Brien, Kevin M.; Olsen, Are; Pierrot, Denis; Rehder, Gregor; Steinhoff, Tobias; Sutton, Adrienne J.; Sweeney, Colm; Tilbrook, Bronte; Wada, Chisato; Wanninkhof, Rik; Akl, John; Barbero, Leticia; Beatty, Cory M.; Berghoff, Carla F.; Bittig, Henry C.; Bott, Randy; Burger, Eugene F.; Cai, Wei-Jun; Castaño-Primo, Rocío; Corredor, Jorge E.; Cronin, Margot; De Carlo, Eric H.; DeGrandpre, Michael D.; Dietrich, Colin; Drennan, William M.; Emerson, Steven R.; Enochs, Ian C.; Enyo, Kazutaka; Epherra, Lucía; Evans, Wiley; Fiedler, Björn; Fontela, Marcos; Frangoulis, Constantin; Gehrung, Martina; Giannoudi, Louisa; Glockzin, Michael; Hales, Burke; Howden, Stephan D.; Ibánhez, J. Severino P.; Kamb, Linus; Körtzinger, Arne; Lefèvre, Nathalie; Lo Monaco, Claire; Lutz, Vivian A.; Macovei, Vlad A.; Maenner Jones, Stacy; Manalang, Dana; Manzello, Derek P.; Metzl, Nicolas; Mickett, John; Millero, Frank J.; Monacci, Natalie M.; Morell, Julio M.; Musielewicz, Sylvia; Neill, Craig; Newberger, Tim; Newton, Jan; Noakes, Scott; Ólafsdóttir, Sólveig Rósa; Ono, Tsuneo; Osborne, John; Padín, Xose A.; Paulsen, Melf; Perivoliotis, Leonidas; Petersen, Wilhelm; Petihakis, George; Plueddemann, Albert J.; Rodriguez, Carmen; Rutgersson, Anna; Sabine, Christopher L.; Salisbury, Joseph E.; Schlitzer, Reiner; Skjelvan, Ingunn; Stamataki, Natalia; Sullivan, Kevin F.; Sutherland, Stewart C.; T'Jampens, Michiel; Tadokoro, Kazuaki; Tanhua, Toste; Telszewski, Maciej; Theetaert, Hannelore; Tomlinson, Michael; Vandemark, Douglas; Velo, Antón; Voynova, Yoana G.; Weller, Robert A.; Whitehead, Chris; Wimart-Rousseau, Cathy (2023). Surface Ocean CO2 Atlas Database Version 2023 (SOCATv2023) (NCEI Accession 0278913). [indicate subset used]. NOAA National Centers for Environmental Information. Dataset. https://doi.org/10.25921/r7xa-bt92. Accessed [25/04/2024].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"doc\":\n",
    "    md(\"Hansell, Dennis A.; Carlson, Craig A.; Amon, Rainer M. W.; Álvarez-Salgado, X. Antón; Yamashita, Youhei; Romera-Castillo, Cristina; Bif, Mariana B. (2021). Compilation of dissolved organic matter (DOM) data obtained from global ocean observations from 1994 to 2021. Version 2 (NCEI Accession 0227166). [indicate subset used]. NOAA National Centers for Environmental Information. Dataset. https://doi.org/10.25921/s4f4-ye35. Accessed [date].\")\n",
    "\n",
    "    md(\"Lønborg, C., Carreira, C., Abril, G., Agustí, S., Amaral, V., Andersson, A., ... & Álvarez-Salgado, X. A. (2024). A global database of dissolved organic matter (DOM) concentration measurements in coastal waters (CoastDOM v1). Earth System Science Data, 16(2), 1107-1119.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"benbio\":\n",
    "    md(\"URL: https://www.vliz.be/vmdcdata/nsbs/about.php\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if test_status:\n",
    "    md(\"This is getting to the end!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
