{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of point_layer template_title using point observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# bin_value using function from r4ecology's github\n",
    "import numpy as np\n",
    "def bin_value(x, bin_res):\n",
    "    return np.floor((x + bin_res / 2) / bin_res + 0.5) * bin_res - bin_res / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "chunk_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "variable = \"point_variable\".lower()\n",
    "vv_name = variable\n",
    "if vv_name.lower() == \"ph\":\n",
    "    vv_name = \"pH\"\n",
    "if vv_name in [\"doc\", \"poc\"]:\n",
    "    vv_name = vv_name.upper()\n",
    "if vv_name == \"benbio\":\n",
    "    vv_name = \"biomass of macrobenthos\"\n",
    "layer = \"point_layer\"\n",
    "# get the units. File inspection could be randomized in case people have put loose files in there...\n",
    "import glob\n",
    "df = pd.read_csv(\"../../matched/mapping.csv\")\n",
    "df = df.query(\"variable == @variable\")\n",
    "pattern = list(df.pattern)[0]\n",
    "# while True:\n",
    "#     i = 0\n",
    "#     patterns = pattern.split(\"/\")\n",
    "#     for x in patterns:\n",
    "#         if x == \"**\":\n",
    "#             break\n",
    "#         i+=1\n",
    "#     new_pattern = glob.glob(\"/\".join(patterns[0:i])+\"/\" + \"**\" )[-1].split(\"/\")[-1]\n",
    "#     patterns[i] = new_pattern\n",
    "#     pattern = \"/\".join(patterns)\n",
    "\n",
    "    \n",
    "#     if len([x for x in pattern.split(\"/\") if x == \"**\"]) == 0:\n",
    "#         break\n",
    "paths = pd.read_csv(glob.glob(f\"../../matched/point/**/{layer}/{variable}/paths.csv\")[0]).path\n",
    "\n",
    "\n",
    "unit = None\n",
    "if unit is None:\n",
    "    try:\n",
    "        ff = glob.glob(f\"../../matched/point/**/{layer}/{variable}/*_{variable}_unit.csv\")[0]\n",
    "        df = pd.read_csv(ff)\n",
    "        unit = df.unit[0]\n",
    "    except:\n",
    "        pass\n",
    "if unit is None:\n",
    "    for ff in paths:\n",
    "        try:\n",
    "            ds = nc.open_data(paths[0])\n",
    "            model_variable = list(df.model_variable)[0].split(\"+\")[0]\n",
    "            unit = list(ds.contents.query(\"variable == @model_variable\").unit)[0]\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "ff = glob.glob(f\"../../matched/point/**/{layer}/{variable}/*_{variable}.csv\")[0]\n",
    "vv_source = os.path.basename(ff).split(\"_\")[0]\n",
    "vv_source = vv_source.upper()\n",
    "df = pd.read_csv(ff)\n",
    "if variable == \"ph\":\n",
    "    df = df.query(\"observation > 4\").reset_index(drop = True)\n",
    "# Danish part is always dubious\n",
    "df = df.query(\"lon < 9\")\n",
    "# ds= nc.open_data(f\"{data_dir}/amm7_val_subdomains.nc\")\n",
    "# ds.subset(variable = \"Shelf\")\n",
    "# ds.as_missing(0)\n",
    "# ds.regrid(df.loc[:,[\"lon\", \"lat\"]].drop_duplicates().reset_index(drop = True), \"nn\")\n",
    "# df_grid = ds.to_dataframe().reset_index().dropna().drop_duplicates()\n",
    "# df = df.merge(df_grid)\n",
    "df_locs = df.loc[:,[\"lon\", \"lat\"]].drop_duplicates()\n",
    "# bin to 0.01 resolution\n",
    "df_raw = df\n",
    "df[\"lon\"] = df[\"lon\"].apply(lambda x: bin_value(x, 0.5))\n",
    "df[\"lat\"] = df[\"lat\"].apply(lambda x: bin_value(x, 0.5))\n",
    "if variable == \"benbio\":\n",
    "    df = df.assign(observation = lambda x: 1000 * 0.45 * x.observation) \n",
    "if variable not in  [\"carbon\", \"benbio\"]:\n",
    "    df = df.groupby([\"lon\", \"lat\", \"year\", \"month\"]).mean().reset_index()\n",
    "else:\n",
    "    df = df.groupby([\"lon\", \"lat\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"carbon\":\n",
    "    md(\"**Note**: This is in progress. Model and observation data are yet to be converted to comparable units!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "if vv_source == \"ices\": \n",
    "\n",
    "    if layer == \"bottom\":\n",
    "        md(f\"Near-bottom values of {vv_name} were extracted from ICES bottle and CTD data.\")\n",
    "    if layer == \"surface\":\n",
    "        md(f\"Values from the top 5 m of the water column were extracted from ICES bottle and CTD data.\")\n",
    "    if layer == \"benthic\":\n",
    "        md(\"Benthic values were extracted from existing datasets\")\n",
    "\n",
    "\n",
    "if layer == \"bottom\":\n",
    "    md(f\"This data was extracted from vertical profiles. The near_bottom value was defined as the value closest to the bottom, that was within 5 m of the bottom. Bathymetry was estimated using GEBCO Bathymetry data.\")\n",
    "if layer == \"surface\":\n",
    "    md(f\"This data was extracted from vertical profiles. Values from the top 5 m were extracted from the database. This was compared with the model values from the surface level.\")\n",
    "if variable in [\"benbio\"]:\n",
    "    md(\"Biomass data for macrobenthos was downloaded from the North Sea Benthos Survey 1986.\")\n",
    "\n",
    "if variable in [\"carbon\"]:\n",
    "    md(\"Carbon data was compiled from multiple sources\")\n",
    "md(f\"In total there were {len(df)} {layer} values extracted from the observational database.\")\n",
    "\n",
    "if layer == \"bottom\":\n",
    "    md(\"**Note:** this analysis has been restricted to observations on the shelf region.\")\n",
    "\n",
    "\n",
    "if variable == \"poc\":\n",
    "    md(\"Particulate organic carbon data was compiled from multiple sources\")\n",
    "\n",
    "if variable == \"pco2\":\n",
    "    md(\"The variable pCO2water_SST_wet was extracted from the SOCAT 2023 database.\")\n",
    "    md(\"Observational values were averaged for each day in the year.\")\n",
    "\n",
    "if variable == \"doc\":\n",
    "    md(\"Dissolved organic carbon data was compiled from multiple sources\")\n",
    "\n",
    "df_mapping = pd.read_csv(\"../../matched/mapping.csv\")\n",
    "model_variable = list(df_mapping.query(\"variable == @variable\").model_variable)[0]\n",
    "\n",
    "md(f\"The following model output was used to compare with observational **{vv_name}**: **{model_variable}**.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# bottom 1% of observations\n",
    "bot_low = df.observation.quantile(0.001)\n",
    "df = df.query(f\"observation >= {bot_low}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i df_locs -i variable -i unit -w 1000 -h 1200\n",
    "library(dplyr, warn.conflicts = FALSE)\n",
    "library(ggplot2, warn.conflicts = FALSE)\n",
    "library(stringr)\n",
    "world_map <- map_data(\"world\")\n",
    "# get lon, lat limits from profile_mld\n",
    "\n",
    "xlim = c(min(df_locs$lon), max(df_locs$lon))\n",
    "ylim = c(min(df_locs$lat), max(df_locs$lat))\n",
    "\n",
    "\n",
    "\n",
    "gg <- df_locs %>%\n",
    "# final six months of the year\n",
    "    ggplot()+\n",
    "    geom_point(aes(lon, lat))+\n",
    "    theme_gray(base_size = 24)+\n",
    "    # add colour scale. Minimum zero, label 100, \">100\"\n",
    "    geom_polygon(data = world_map, aes(long, lat, group = group), fill = \"grey60\")+\n",
    "    coord_fixed(xlim = xlim, ylim = ylim, ratio = 1.5) \n",
    "\n",
    "# figure out if lon minimum is less than -10\n",
    "if( min(df_locs$lon) < -10 ){\n",
    "    # add sensible labels for longitude and latitude\n",
    "\n",
    "    gg <- gg +\n",
    "    scale_x_continuous(breaks = seq(-10, 5, 5), labels = c(\"10°W\", \"5°W\", \"0°\", \"5°E\"))+ \n",
    "    scale_y_continuous(breaks = seq(45, 60, 5), labels = c(\"45°N\", \"50°N\", \"55°N\", \"60°N\"))+\n",
    "    labs(x = \"\", y = \"\") \n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "    # move legen\n",
    "\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"**Figure {i_figure}:** Map of {layer} {vv_name} observations from {vv_source}.\")\n",
    "i_figure = i_figure + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i df -i variable -i unit -w 1000 -h 1200\n",
    "library(tidyverse)\n",
    "if ((\"month\" %in% colnames(df)) == FALSE){\n",
    "\n",
    "df_map <- df %>%\n",
    "    gather(variable, value, model:observation)\n",
    "    df_map\n",
    "# calculate the 98th percentil of the data\n",
    "p98 = quantile(df_map$value, 0.98)\n",
    "# cap the value at this\n",
    "df_map$value = pmin(df_map$value, p98)\n",
    "\n",
    "world_map <- map_data(\"world\")\n",
    "\n",
    "gg <- df_map %>%\n",
    "    ggplot()+\n",
    "    geom_tile(aes(lon, lat, fill = value))+\n",
    "    theme_gray(base_size = 24)+\n",
    "    coord_fixed(ratio = 1.5, xlim = c(min(df$lon), max(df$lon)), ylim = c(min(df$lat), max(df$lat)))+\n",
    "    labs(color = variable)+\n",
    "    geom_polygon(data = world_map, aes(long, lat, group = group), fill = \"grey60\")+\n",
    "    # log10\n",
    "    scale_color_viridis_c()+\n",
    "    theme(legend.position = \"bottom\")+\n",
    "    facet_wrap(~variable)+\n",
    "      scale_fill_viridis_c(\n",
    "                       guide = guide_colorbar(title.position = \"bottom\", title.hjust = 0.5, title.theme = element_text(angle = 0, size = 20, family = \"Helvetica\"))\n",
    "  )+\n",
    "    theme(\n",
    "    legend.position = \"bottom\", legend.direction = \"horizontal\", legend.box = \"horizontal\", legend.key.width = unit(6.0, \"cm\"),\n",
    "    legend.key.height = unit(1.0, \"cm\"))\n",
    "\n",
    "# ditch the whitespace around the plot\n",
    "gg <- gg + theme(plot.margin=unit(c(0,0,0,0),\"cm\"))\n",
    "gg\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if \"month\" not in df.columns:\n",
    "    md(f\"**Figure {i_figure}:** Map of average {layer} {vv_name} in the model and observational datasets.\")\n",
    "    i_figure += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i df -i variable -i unit -w 1000 -h 1200\n",
    "# calculate number of observations per month\n",
    "# figure out if \"month\" in df\n",
    "if(\"month\" %in% colnames(df)){\n",
    "\n",
    "df1 <- df %>%\n",
    "    group_by(lon, lat, month) %>%\n",
    "    summarise(observation = n()) %>%\n",
    "    ungroup()   \n",
    "\n",
    "# plot number of observations per month using plotnine and geom_bar\n",
    "\n",
    "# change month to month name\n",
    "df1$month <- factor(df1$month, levels = 1:12, labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))\n",
    "\n",
    "gg <- df1 %>%\n",
    "    ggplot(aes(x = month, y = observation))+\n",
    "    theme_gray(base_size = 24)+\n",
    "    geom_bar(stat = \"identity\")+\n",
    "    labs(y = \"Number of observations\", x= \"\")\n",
    "\n",
    "gg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if \"month\" in df.columns:\n",
    "    md(f\"**Figure {i_figure}:** Number of {layer} observations per month for {vv_name}.\")\n",
    "    i_figure += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i df -i variable -i unit -i layer -w 1000 -h 1200 -i vv_name\n",
    "#%%R -i df -i variable -i unit -w 1600 -h 1000\n",
    "\n",
    "library(dplyr, warn.conflicts = FALSE)\n",
    "library(ggplot2, warn.conflicts = FALSE)\n",
    "library(stringr)\n",
    "library(tidyverse)\n",
    "world_map <- map_data(\"world\")\n",
    "# get lon, lat limits from profile_mld\n",
    "\n",
    "xlim = c(min(df$lon), max(df$lon))\n",
    "ylim = c(min(df$lat), max(df$lat))\n",
    "\n",
    "\n",
    "df <- df %>%\n",
    "    mutate(bias = model - observation) \n",
    "\n",
    "# calculate the absolate bias\n",
    "\n",
    "df1 <- df %>%\n",
    "    mutate(bias = abs(bias))\n",
    "# calculate the 98th percentile of the absolute bias\n",
    "bias_high <- df1$bias %>% quantile(0.98)\n",
    "# cap the bias to +/1 98th percentile\n",
    "df$bias[df$bias > bias_high] <- bias_high\n",
    "df$bias[df$bias < -bias_high] <- -bias_high\n",
    "\n",
    "\n",
    "\n",
    "plot_month <- FALSE\n",
    "if(\"month\" %in% colnames(df))\n",
    "    plot_month <- TRUE\n",
    "\n",
    "# # convert month number to month in profile_mld\n",
    "if(plot_month){\n",
    "    df <- df %>%\n",
    "        arrange(month)\n",
    "df$month <- factor(df$month, levels = df$month, labels = month.abb[df$month])\n",
    "}\n",
    "# df$month <- factor(df$month, labels = month.abb)\n",
    "\n",
    "title <- str_glue(\"Bias in {layer} {vv_name} ({unit})\")\n",
    "\n",
    "out = str_glue(\"../../results/{layer}/{variable}/{layer}_{variable}_bias.csv\")\n",
    "\n",
    "# # check directory exists for out\n",
    "if (!dir.exists(dirname(out))){\n",
    "    dir.create(dirname(out), recursive = TRUE)\n",
    "}\n",
    "df %>% write_csv(out)\n",
    "\n",
    "# df.to_csv(out, index = False)\n",
    "\n",
    "# export to csv\n",
    "\n",
    "\n",
    "\n",
    "gg <- df %>%\n",
    "# final six months of the year\n",
    "    ggplot()+\n",
    "    geom_raster(aes(lon, lat, fill = bias))+\n",
    "    theme_gray(base_size = 24)+\n",
    "    # add colour scale. Minimum zero, label 100, \">100\"\n",
    "    geom_polygon(data = world_map, aes(long, lat, group = group), fill = \"grey60\")+\n",
    "    coord_fixed(xlim = xlim, ylim = ylim, ratio = 1.5) +\n",
    "    # move legend to the top. Make it 3 cm wide\n",
    "    # move legend title to the bottom and centre it\n",
    "    scale_fill_gradient2(low = \"blue\", high = \"red\",\n",
    "                       guide = guide_colorbar(title.position = \"bottom\", title.hjust = 0.5, title.theme = element_text(angle = 0, size = 20, family = \"Helvetica\"))\n",
    "  )+\n",
    "    theme(\n",
    "    legend.position = \"bottom\", legend.direction = \"horizontal\", legend.box = \"horizontal\", legend.key.width = unit(6.0, \"cm\"),\n",
    "    legend.key.height = unit(1.0, \"cm\"))+\n",
    "    # set the legend title to bias\n",
    "    labs(fill = title)\n",
    "if (plot_month){\n",
    "    gg <- gg + facet_wrap(~month)\n",
    "}\n",
    "\n",
    "# figure out if lon minimum is less than -10\n",
    "if( min(df$lon) < -10 ){\n",
    "    # add sensible labels for longitude and latitude\n",
    "\n",
    "    gg <- gg +\n",
    "    scale_x_continuous(breaks = seq(-10, 5, 5), labels = c(\"10°W\", \"5°W\", \"0°\", \"5°E\"))+ \n",
    "    scale_y_continuous(breaks = seq(45, 60, 5), labels = c(\"45°N\", \"50°N\", \"55°N\", \"60°N\"))+\n",
    "    labs(x = \"\", y = \"\") \n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "    # move legen\n",
    "\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"**Figure {i_figure}**: Bias in {layer} {vv_name}. The bias is calculated as model - observation. The colour scale is from blue (negative bias) to red (positive bias). The colour scale is capped at the 98th percentile of the absolute bias. This is to avoid a few extreme outliers from dominating the colour scale. **Note:** values have been binned and averaged to 0.5 degree resolution.\") \n",
    "i_figure += 1\n",
    "\n",
    "#\"adhoc/tmp/df_raw.feather\"\n",
    "# create directory if non-existent, recursive\n",
    "if os.path.isdir(\"adhoc/tmp\") == False:\n",
    "    os.makedirs(\"adhoc/tmp\")\n",
    "df_raw.to_feather(\"adhoc/tmp/df_raw.feather\")\n",
    "df.to_feather(\"adhoc/tmp/df.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i vv_name -i unit -w 1000 -h 1200\n",
    "#%%R -i df -i variable -i unit -w 1600 -h 1000\n",
    "df <- arrow::read_feather(\"adhoc/tmp/df_raw.feather\")\n",
    "if(\"month\" %in% colnames(df)){\n",
    "\n",
    "\n",
    "library(tidyverse, warn.conflicts = FALSE)\n",
    "\n",
    "\n",
    "x_lab <- str_glue(\"Model {vv_name} ({unit})\")\n",
    "y_lab <- str_glue(\"Observed {vv_name} ({unit})\")\n",
    "\n",
    "\n",
    "gg <- df %>%\n",
    "# final six months of the year\n",
    "    ggplot()+\n",
    "    geom_point(aes(model, observation))+\n",
    "    facet_wrap(~month)+\n",
    "    theme_gray(base_size = 24)+\n",
    "    labs(fill = title)+\n",
    "    geom_abline()+\n",
    "    geom_smooth(aes(model, observation), method = \"gam\")+\n",
    "    labs(x = x_lab, y = y_lab)\n",
    "    # move legen\n",
    "\n",
    "gg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    md(f\"**Figure {i_figure}**: Model vs observed {vv_name} for {layer} observations. The observations are from {vv_source}. The line is a GAM fit to the data. The data has been binned to 0.5 degree resolution.\")\n",
    "    i_figure = i_figure + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%R -i vv_name -i unit -w 1000 -h 1200\n",
    "#%%R -i df -i variable -i unit -w 1600 -h 1000\n",
    "\n",
    "library(dplyr, warn.conflicts = FALSE)\n",
    "library(ggplot2, warn.conflicts = FALSE)\n",
    "library(stringr)\n",
    "\n",
    "\n",
    "df <- arrow::read_feather(\"adhoc/tmp/df.feather\")\n",
    "\n",
    "\n",
    "\n",
    "x_lab <- str_glue(\"Model {vv_name} ({unit})\")\n",
    "y_lab <- str_glue(\"Observed {vv_name} ({unit})\")\n",
    "\n",
    "\n",
    "gg <- df %>%\n",
    "# final six months of the year\n",
    "    ggplot()+\n",
    "    geom_point(aes(model, observation))+\n",
    "    theme_gray(base_size = 24)+\n",
    "    labs(fill = title)+\n",
    "    geom_abline()+\n",
    "    geom_smooth(aes(model, observation), method = \"gam\")+\n",
    "    labs(x = x_lab, y = y_lab)\n",
    "    # move legen\n",
    "\n",
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"**Figure {i_figure}**: Model vs observed {vv_name} for {layer} values. The observations are from {vv_source}. The line is a GAM fit to the data. The shaded area is the 95% confidence interval of the GAM fit.\")\n",
    "i_figure += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"The overall ability of the model to predict the observed {vv_name} was assessed by calculating the average bias, the root mean square error (RMSE) and the correlation coefficient (R). The bias was calculated as the model value minus the observed value. The RMSE was calculated as the square root of the mean squared error. The correlation coefficient was calculated as the Pearson correlation coefficient between the model and observed values.\") \n",
    "md(f\"This was calculated for each month and for the entire dataset. The results are shown in the tables below.\")\n",
    "md(f\"This is calculated in two separate ways. First, we use the raw model and observed values. Second, we use data that was averaged to 0.5 to 0.5 bins to account for spatial bias.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_bias = (\n",
    "        df_raw\n",
    "        .assign(bias = lambda x: x.model - x.observation)\n",
    "        .groupby(\"month\")\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .loc[:,[\"month\", \"bias\"]]\n",
    "        # convert month number to name\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "    )\n",
    "    # add average bias to df_bias as a separate row\n",
    "    annual_bias = df_raw.model.mean() - df_raw.observation.mean() \n",
    "    df_bias = pd.concat([df_bias, pd.DataFrame({\"month\": [\"All\"], \"bias\": [annual_bias]})])\n",
    "\n",
    "    # move the final row to the top\n",
    "    df_bias = pd.concat([df_bias.iloc[[-1]], df_bias.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_bias = pd.DataFrame({\"month\": [\"All\"], \"bias\": [df_raw.model.mean() - df_raw.observation.mean()]})\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    # now create an rmse dataframe\n",
    "    df_rmse = (\n",
    "        df_raw\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "        .groupby(\"month\")\n",
    "        .apply(lambda x: np.sqrt((x.model - x.observation).pow(2).mean()))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"rmse\"})\n",
    "    )\n",
    "    # add average rmse to df_rmse as a separate row\n",
    "    annual_rmse = np.sqrt(((df_raw.model - df_raw.observation).pow(2)).mean())\n",
    "    df_rmse = pd.concat([df_rmse, pd.DataFrame({\"month\": [\"All\"], \"rmse\": [annual_rmse]})])\n",
    "    # move the final row to the top\n",
    "    df_rmse = pd.concat([df_rmse.iloc[[-1]], df_rmse.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_rmse = pd.DataFrame({\"month\": [\"All\"], \"rmse\": [np.sqrt(((df_raw.model - df_raw.observation).pow(2)).mean())]})\n",
    "# rename the month column to Month\n",
    "# merge the two dataframes\n",
    "df_table = copy.deepcopy(df_bias).merge(df_rmse)\n",
    "df_table = df_table.round(2)\n",
    "# create df_corr\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_corr = (\n",
    "        df_raw\n",
    "        .groupby(\"month\")\n",
    "        .apply(lambda x: x.model.corr(x.observation))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"correlation\"})\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "    )\n",
    "    # add average correlation to df_corr as a separate row\n",
    "    # calculate annual correlation using all data\n",
    "    annual_corr = df_raw.model.corr(df_raw.observation)\n",
    "    df_corr = pd.concat([df_corr, pd.DataFrame({\"month\": [\"All\"], \"correlation\": [annual_corr]})])\n",
    "    # df_corr = df_corr.append({\"month\": \"All\", \"correlation\": annual_corr}, ignore_index=True)\n",
    "\n",
    "    # move the final row to the top\n",
    "    df_corr = pd.concat([df_corr.iloc[[-1]], df_corr.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_corr = pd.DataFrame({\"month\": [\"All\"], \"correlation\": [df_raw.model.corr(df_raw.observation)]})\n",
    "df_table = df_table.merge(df_corr)\n",
    "df_table = df_table.round(2)\n",
    "df_table = df_table.rename(columns={\"month\": \"Month\", \"bias\": \"Bias\", \"rmse\": \"RMSE\", \"correlation\": \"Correlation\"})\n",
    "df_table = df_table[[\"Month\", \"Bias\", \"RMSE\", \"Correlation\"]]\n",
    "# change Month to Period\n",
    "df_table = df_table.rename(columns={\"Month\": \"Time period\"})\n",
    "\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    # add commas to bias and rmse\n",
    "    df_number = df_raw.groupby(\"month\").count().reset_index().loc[:,[\"month\", \"observation\"]]\n",
    "# convert month number to name\n",
    "    df_number[\"month\"] = df_number[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "    df_number = df_number.rename(columns={\"month\": \"Time period\", \"observation\": \"Number of observations\"})\n",
    "else:\n",
    "    df_number = pd.DataFrame({\"Time period\": [\"All\"], \"Number of observations\": [len(df_raw)]})\n",
    "\n",
    "# add total number of observations\n",
    "annual_number = len(df_raw)\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_number = pd.concat([df_number, pd.DataFrame({\"Time period\": [\"All\"], \"Number of observations\": [annual_number]})])\n",
    "# df_number = df_number.append({\"Time period\": \"All\", \"Number of observations\": annual_number}, ignore_index=True)\n",
    "df_table = df_table.merge(df_number)\n",
    "\n",
    "# include commas in the number of observations\n",
    "df_table[\"Number of observations\"] = df_table[\"Number of observations\"].apply(lambda x: \"{:,}\".format(x))\n",
    "\n",
    "df_table.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"**Table {i_table}:** Average bias and root-mean square error in {layer} {vv_name} for each month using the raw {vv_source} data. The bias is calculated as model - observation. The average bias is calculated as the mean of the monthly biases.\")\n",
    "i_table += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_bias = (\n",
    "        df\n",
    "        .assign(bias = lambda x: x.model - x.observation)\n",
    "        .groupby(\"month\")\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .loc[:,[\"month\", \"bias\"]]\n",
    "        # convert month number to name\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "    )\n",
    "    # add average bias to df_bias as a separate row\n",
    "    annual_bias = df.model.mean() - df.observation.mean() \n",
    "    df_bias = pd.concat([df_bias, pd.DataFrame({\"month\": [\"All\"], \"bias\": [annual_bias]})])\n",
    "\n",
    "    # move the final row to the top\n",
    "    df_bias = pd.concat([df_bias.iloc[[-1]], df_bias.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_bias = pd.DataFrame({\"month\": [\"All\"], \"bias\": [df.model.mean() - df.observation.mean()]})\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    # now create an rmse dataframe\n",
    "    df_rmse = (\n",
    "        df\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "        .groupby(\"month\")\n",
    "        .apply(lambda x: np.sqrt((x.model - x.observation).pow(2).mean()))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"rmse\"})\n",
    "    )\n",
    "    # add average rmse to df_rmse as a separate row\n",
    "    annual_rmse = np.sqrt(((df.model - df.observation).pow(2)).mean())\n",
    "    df_rmse = pd.concat([df_rmse, pd.DataFrame({\"month\": [\"All\"], \"rmse\": [annual_rmse]})])\n",
    "    # move the final row to the top\n",
    "    df_rmse = pd.concat([df_rmse.iloc[[-1]], df_rmse.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_rmse = pd.DataFrame({\"month\": [\"All\"], \"rmse\": [np.sqrt(((df.model - df.observation).pow(2)).mean())]})\n",
    "# rename the month column to Month\n",
    "# merge the two dataframes\n",
    "df_table = copy.deepcopy(df_bias).merge(df_rmse)\n",
    "df_table = df_table.round(2)\n",
    "# create df_corr\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_corr = (\n",
    "        df\n",
    "        .groupby(\"month\")\n",
    "        .apply(lambda x: x.model.corr(x.observation))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"correlation\"})\n",
    "        .assign(month = lambda x: x.month.apply(lambda y: calendar.month_abbr[y]))\n",
    "    )\n",
    "    # add average correlation to df_corr as a separate row\n",
    "    # calculate annual correlation using all data\n",
    "    annual_corr = df.model.corr(df.observation)\n",
    "    df_corr = pd.concat([df_corr, pd.DataFrame({\"month\": [\"All\"], \"correlation\": [annual_corr]})])\n",
    "    # df_corr = df_corr.append({\"month\": \"All\", \"correlation\": annual_corr}, ignore_index=True)\n",
    "\n",
    "    # move the final row to the top\n",
    "    df_corr = pd.concat([df_corr.iloc[[-1]], df_corr.iloc[:-1]])\n",
    "else:\n",
    "    # only want annual\n",
    "    df_corr = pd.DataFrame({\"month\": [\"All\"], \"correlation\": [df.model.corr(df.observation)]})\n",
    "df_table = df_table.merge(df_corr)\n",
    "df_table = df_table.round(2)\n",
    "df_table = df_table.rename(columns={\"month\": \"Month\", \"bias\": \"Bias\", \"rmse\": \"RMSE\", \"correlation\": \"Correlation\"})\n",
    "df_table = df_table[[\"Month\", \"Bias\", \"RMSE\", \"Correlation\"]]\n",
    "# change Month to Period\n",
    "df_table = df_table.rename(columns={\"Month\": \"Time period\"})\n",
    "\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    # add commas to bias and rmse\n",
    "    df_number = df.groupby(\"month\").count().reset_index().loc[:,[\"month\", \"observation\"]]\n",
    "# convert month number to name\n",
    "    df_number[\"month\"] = df_number[\"month\"].apply(lambda x: calendar.month_abbr[x])\n",
    "    df_number = df_number.rename(columns={\"month\": \"Time period\", \"observation\": \"Number of observations\"})\n",
    "else:\n",
    "    df_number = pd.DataFrame({\"Time period\": [\"All\"], \"Number of observations\": [len(df)]})\n",
    "\n",
    "# add total number of observations\n",
    "annual_number = len(df)\n",
    "if variable not in [\"carbon\", \"benbio\"]:\n",
    "    df_number = pd.concat([df_number, pd.DataFrame({\"Time period\": [\"All\"], \"Number of observations\": [annual_number]})])\n",
    "# df_number = df_number.append({\"Time period\": \"All\", \"Number of observations\": annual_number}, ignore_index=True)\n",
    "df_table = df_table.merge(df_number)\n",
    "\n",
    "# include commas in the number of observations\n",
    "df_table[\"Number of observations\"] = df_table[\"Number of observations\"].apply(lambda x: \"{:,}\".format(x))\n",
    "\n",
    "df_table.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"**Table {i_table}:** Average bias and root-mean square error in {layer} {vv_name} for each month using binned {vv_source} data. Data was averaged in each 0.5 by 0.5 degree cell in each year and month. The bias is calculated as model - observation. The average bias is calculated as the mean of the monthly biases.\")\n",
    "i_table += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regresion analysis of model vs observed point_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "md(f\"A linear regression analysis of modelled and observed {vv_name} was performed. The modelled {vv_name} was used as the independent variable and the observed {vv_name} was used as the dependent variable. The results are shown in the table below.\")\n",
    "\n",
    "md(\"The regression was carried out using the Python package statsmodels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# do a linear regression of model vs observed in df\n",
    "X = df.model.values\n",
    "Y = df.observation.values\n",
    "# linear regression using statsmodels\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X)\n",
    "# make X and Y random numbers between 0 and 1\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X).fit()\n",
    "# get the slope and intercept\n",
    "intercept, slope = model.params\n",
    "# calculate the r squared\n",
    "r2 = model.rsquared\n",
    "# calculate the p value of the slope\n",
    "p = model.f_pvalue\n",
    "\n",
    "p = model.f_pvalue\n",
    "# put that in a dataframe\n",
    "df_stats = pd.DataFrame({\"Slope\": slope, \"Intercept\": intercept, \"R2\": r2, \"P\": p}, index = [\"All\"]).assign(Period = \"All\")\n",
    "# do this month by month append to df_stats\n",
    "\n",
    "for month in range(1, 13):\n",
    "    try:\n",
    "        X = df.query(\"month == @month\").model.values\n",
    "        Y = df.query(\"month == @month\").observation.values\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        intercept, slope = model.params\n",
    "        r2 = model.rsquared\n",
    "        p = model.f_pvalue\n",
    "        df_stats = pd.concat([df_stats, pd.DataFrame({\"Slope\": slope, \"Intercept\": intercept, \"R2\": r2, \"P\": p}, index = [month]).assign(Period = month)])\n",
    "        df_stats.loc[df_stats.index[-1], \"Period\"] = calendar.month_abbr[month]\n",
    "    except:\n",
    "        pass\n",
    "# sort period appropriately, so All is first then ordered by month\n",
    "df_stats[\"Period\"] = pd.Categorical(df_stats[\"Period\"], [calendar.month_abbr[x] for x in range(1, 13)] + [\"All\"])\n",
    "# round p-value to 3 dp\n",
    "df_stats[\"P\"] = df_stats[\"P\"].round(5)\n",
    "# change P to p-value\n",
    "df_stats = df_stats.rename(columns={\"P\": \"p-value\"})\n",
    "# put Period first\n",
    "df_stats = df_stats[[\"Period\", \"Slope\", \"Intercept\", \"R2\", \"p-value\"]]\n",
    "# \n",
    "\n",
    "df_stats.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "chunk_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "## Data citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"poc\":\n",
    "    md(\"Boss, Emmanuel; Picheral, Marc; Searson, Sarah; Le Goff, Hervé; Reverdin, Gilles; Leeuw, Thomas; Chase, Alison P; Bricaud, Annick; Kolber, Zbigniew S; Taillandier, V; Pesant, Stephane; Tara Oceans Consortium, Coordinators; Tara Oceans Expedition, Participants (2017): Underway surface water data during the Tara Oceans expedition in 2009-2012 [dataset]. PANGAEA, https://doi.org/10.1594/PANGAEA.873566, In: Boss, Emmanuel; Picheral, Marc; Searson, Sarah; Marec, Claudie; Le Goff, Hervé; Reverdin, Gilles; Leeuw, Thomas; Chase, Alison P; Anderson, Leif G; Gattuso, Jean-Pierre; Pino, Diana Ruiz; Padín, Xose Antonio; Grondin, Pierre-Luc; Matuoka, Atsushi; Babin, Marcel; Bricaud, Annick; Kolber, Zbigniew S; Taillandier, V; Hafez, Mark; Chekalyuk, Alexander; Pesant, Stephane; Météo France; Tara Oceans Consortium, Coordinators (2017): Harmonised data from underway navigation, meteorology and surface water measurements during the Tara Oceans expedition in 2009-2013 [dataset publication series]. PANGAEA, https://doi.org/10.1594/PANGAEA.873592\")\n",
    "    md(\"Röttgers, Rüdiger; Bi, Shun; Burmester, Henning; Heymann, Kerstin; Hieronymi, Martin; Krasemann, Hajo; Schönfeld, Wolfgang (2023): Water inherent optical properties and concentrations of water constituents from the German Bight and adjacent regions: concentrations and auxiliary data [dataset]. PANGAEA, https://doi.org/10.1594/PANGAEA.950767, In: Röttgers, Rüdiger; Bi, Shun; Burmester, Henning; Heymann, Kerstin; Hieronymi, Martin; Krasemann, Hajo; Schönfeld, Wolfgang (2023): Water inherent optical properties and concentrations of water constituents from the German Bight and adjacent regions [dataset bundled publication]. PANGAEA, https://doi.org/10.1594/PANGAEA.950774)\")\n",
    "    md(\"Loisel, Hubert; Duforêt-Gaurier, Lucile; Tran, Trung Kien; Jorge, Daniel S F; Steinmetz, Francois; Mangin, Antoine; Bretagnon, Marine; d'Andon, Odile (2023): Database (DSM) of in situPOC, SPM and Rrs collected between 1997 and 2018 [dataset]. PANGAEA, https://doi.org/10.1594/PANGAEA.960962\")\n",
    "    md(\"Lønborg, Christian; Carreira, Cátia; Abril, Gwenael; Agustí, Susana; Amaral, Valentina; Andersson, Agneta; Arístegui, Javier; Bhadury, Punyasloke; Bif, Mariana B; Borges, Alberto Vieira; Bouillon, Steven; Calleja, Maria Ll; Cotovicz, Luiz C Jr; Cozzi, Stefano; Doval, Maryló; Duarte, Carlos Manuel; Eyre, Bradley D; Fichot, Cedric; García-Martín, Elena; Garzon-Garcia, Alexandra; Giani, Michele; Gonçalves-Araujo, Rafael; Gruber, Renee K; Hansell, Dennis A; Hashihama, Fuminori; He, Ding; Holding, Johnna M; Hunter, William Ross; Ibánhez, J Severino; Ibello, Valeria; Jiang, Shan; Kim, Guebuem; Klun, Katja; Kowalczuk, Piotr; Kubo, Atsushi; Lee, Choon Weng; Lopes, Claudia B; Maggioni, Federica; Magni, Paolo; Marrasé, Celia; Martin, Patrick; McCallister, S Leigh; McCallum, Rosh; M Medeiros, Patricia; G Morán, Xosé Anxelu; Muller-Karger, Frank; Myers-Pigg, Allison; Norli, Marit; Oakes, Joanne M; Osterholz, Helena; Park, Hyekyung; Lund Paulsen, Maria; Rosentreter, Judith A; Ross, Jeff; Rueda-Roa, Digna; Santinelli, Chiara; Shen, Yuan; Teira, Eva; Tinta, Tinkara; Uher, Guenther; Wakita, Masahide; Ward, Nicholas D; Watanabe, Kenta; Xin, Yu; Yamashita, Youhei; Yang, Liyang; Yeo, Jacob; Yuan, Huamao; Zheng, Qiang; Álvarez‐Salgado, Xosé Antón (2023): A global database of dissolved organic matter (DOM) concentration measurements in coastal waters (CoastDOM v.1) [dataset]. PANGAEA, https://doi.org/10.1594/PANGAEA.964012\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if vv_source == \"ices\":\n",
    "    md(f\"Data for {vv_name} was downloaded from the ICES website on 3rd March 2023. The data is available from [ICES](https://data.ices.dk/view-map).\") \n",
    "    md(\"A minimum requirement citation would include the following: “ICES Data Portal, Dataset on Ocean Hydrochemistry, 2023. ICES, Copenhagen”\")\n",
    "\n",
    "    md(\"Extended citation: ICES Data Portal, Dataset on Ocean HydroChemistry, Extracted March 3, 2023. ICES, Copenhagen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"carbon\":\n",
    "    md('Diesing, Markus, Terje Thorsnes, and Lilja Rún Bjarnadóttir. \"Organic carbon densities and accumulation rates in surface sediments of the North Sea and Skagerrak.\" Biogeosciences 18.6 (2021): 2139-2160.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if vv_source == \"socat23\":\n",
    "    md(\"Bakker, Dorothee C. E.; Alin, Simone R.; Bates, Nicholas; Becker, Meike; Feely, Richard A.; Gkritzalis, Thanos; Jones, Steve D.; Kozyr, Alex; Lauvset, Siv K.; Metzl, Nicolas; Munro, David R.; Nakaoka, Shin-ichiro; Nojiri, Yukihiro; O'Brien, Kevin M.; Olsen, Are; Pierrot, Denis; Rehder, Gregor; Steinhoff, Tobias; Sutton, Adrienne J.; Sweeney, Colm; Tilbrook, Bronte; Wada, Chisato; Wanninkhof, Rik; Akl, John; Barbero, Leticia; Beatty, Cory M.; Berghoff, Carla F.; Bittig, Henry C.; Bott, Randy; Burger, Eugene F.; Cai, Wei-Jun; Castaño-Primo, Rocío; Corredor, Jorge E.; Cronin, Margot; De Carlo, Eric H.; DeGrandpre, Michael D.; Dietrich, Colin; Drennan, William M.; Emerson, Steven R.; Enochs, Ian C.; Enyo, Kazutaka; Epherra, Lucía; Evans, Wiley; Fiedler, Björn; Fontela, Marcos; Frangoulis, Constantin; Gehrung, Martina; Giannoudi, Louisa; Glockzin, Michael; Hales, Burke; Howden, Stephan D.; Ibánhez, J. Severino P.; Kamb, Linus; Körtzinger, Arne; Lefèvre, Nathalie; Lo Monaco, Claire; Lutz, Vivian A.; Macovei, Vlad A.; Maenner Jones, Stacy; Manalang, Dana; Manzello, Derek P.; Metzl, Nicolas; Mickett, John; Millero, Frank J.; Monacci, Natalie M.; Morell, Julio M.; Musielewicz, Sylvia; Neill, Craig; Newberger, Tim; Newton, Jan; Noakes, Scott; Ólafsdóttir, Sólveig Rósa; Ono, Tsuneo; Osborne, John; Padín, Xose A.; Paulsen, Melf; Perivoliotis, Leonidas; Petersen, Wilhelm; Petihakis, George; Plueddemann, Albert J.; Rodriguez, Carmen; Rutgersson, Anna; Sabine, Christopher L.; Salisbury, Joseph E.; Schlitzer, Reiner; Skjelvan, Ingunn; Stamataki, Natalia; Sullivan, Kevin F.; Sutherland, Stewart C.; T'Jampens, Michiel; Tadokoro, Kazuaki; Tanhua, Toste; Telszewski, Maciej; Theetaert, Hannelore; Tomlinson, Michael; Vandemark, Douglas; Velo, Antón; Voynova, Yoana G.; Weller, Robert A.; Whitehead, Chris; Wimart-Rousseau, Cathy (2023). Surface Ocean CO2 Atlas Database Version 2023 (SOCATv2023) (NCEI Accession 0278913). [indicate subset used]. NOAA National Centers for Environmental Information. Dataset. https://doi.org/10.25921/r7xa-bt92. Accessed [25/04/2024].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"doc\":\n",
    "    md(\"Hansell, Dennis A.; Carlson, Craig A.; Amon, Rainer M. W.; Álvarez-Salgado, X. Antón; Yamashita, Youhei; Romera-Castillo, Cristina; Bif, Mariana B. (2021). Compilation of dissolved organic matter (DOM) data obtained from global ocean observations from 1994 to 2021. Version 2 (NCEI Accession 0227166). [indicate subset used]. NOAA National Centers for Environmental Information. Dataset. https://doi.org/10.25921/s4f4-ye35. Accessed [date].\")\n",
    "\n",
    "    md(\"Lønborg, C., Carreira, C., Abril, G., Agustí, S., Amaral, V., Andersson, A., ... & Álvarez-Salgado, X. A. (2024). A global database of dissolved organic matter (DOM) concentration measurements in coastal waters (CoastDOM v1). Earth System Science Data, 16(2), 1107-1119.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if variable == \"benbio\":\n",
    "    md(\"URL: https://www.vliz.be/vmdcdata/nsbs/about.php\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
